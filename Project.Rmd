---
title: "DSCI 100 Project Proposal"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---

### Import Data

Predictive Question: Can we predict the popularity (# number of shares ) given .....(predictors of the data)
```{r Load Libraries, message=FALSE, warning=FALSE}
library(tidyr)
library(repr)
library(rsample)
library(GGally)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(kknn)
```

```{r}
news <- read.csv("~/Desktop/DSCI100 Project /OnlineNewsPopularity.csv")
# temp <- tempfile()
# temp_dir <- tempfile()
# download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip", temp, mode="wb")
# OnlineNewsPopularity_unzipped <- unzip(temp, exdir = temp_dir)
# 
# #read the csv within the unzipped_contents
# online_news_pop_csv <- OnlineNewsPopularity_unzipped[2] #may be unzipped_contents[1] if you get an error
# online_news_pop_data <- read_csv(online_news_pop_csv)
# online_news_pop_data
# unlink(c(temp, temp_dir))
```


### Wrangle and clean

Creating a categorical variable to represent the popularity (shares) so it help us better visualize the data. The dividing criteria comes from the summary of the shares
```{r echo=FALSE, out.width= "33%"}
options(repr.plot.width = 30, repr.plot.height = 10)
summary(news$shares)
boxplot(news$shares, outline = FALSE, horizontal = TRUE)
title(main = "Distribution of Shares", xlab = "Number of Shares")
```

```{r}
news = news %>% 
      mutate(popularity = ifelse(shares < 500, "Not_Popular",
                          ifelse(shares %in% 500:1399, "Somewhat_Popular",
                          ifelse(shares %in% 1400:3299, "Popular", "Viral")))) %>% 
      mutate(popularity = as.factor(popularity))
```


```{r}
# combining the weekdays and data channels into one column

# We set all 0 to NA here, because in the original data set, 0 is used to represent FALSE. 
# Therefore, to get rid of all FALSE values, we set them to NA so that in pivot_longer, 
# we can remove these unecessary data points using value_drop_na = TRUE
news[,14:19][news[,14:19] == 0] = NA

news = news %>% 
    pivot_longer(data_channel_is_lifestyle:data_channel_is_world,
                 names_to = "Channel",
                 names_prefix = "data_channel_is_",
                 values_to = "Value",
                 values_drop_na = TRUE) %>% 
  select(-Value)

# Same thing here, setting 0 values to NA to get rid of them
news[,32:38][news[32:38] == 0] = NA
news = news %>% 
    pivot_longer(weekday_is_monday:weekday_is_sunday,
                 names_to = "weekday",
                 names_prefix = "weekday_is_",
                 values_to = "value",
                 values_drop_na = TRUE) %>% 
  select(-value)

head(news$weekday, 21)
```


### Exploratory data analysis
By ref
```{r}
splitted_data <- initial_split(news, prop = 0.80, strata = popularity)  
training_set <- training(splitted_data)   
testing_set <- testing(splitted_data)
# training_set
# testing_set

# https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio
```

```{r}
## To filter out only quantitative varibles
news_numerical = unlist(lapply(training_set, is.numeric)) 
news_numerical = training_set[ , news_numerical]

cor(news_numerical) %>%
  as_data_frame(rownames = "var1") %>%
    pivot_longer(cols = -var1, names_to = "var2", values_to = "coeff") %>%
    filter(var1 == "shares") %>% 
    filter(var1 != var2) %>%
    arrange(desc(abs(coeff)))

## Code reference:
## https://stackoverflow.com/questions/48123611/using-ggpairs-on-a-large-dataset-with-many-variables
```
Based on the above, we choose kw_avg_avg, self_reference_avg_sharess, and num_hrefs.
That is, the Average keyword (avg. shares), the Avg. shares of referenced articles in Mashable, and the Number of links as they have the highest correlation with shares.



By filtering out the missing values, we successfully increased the correlation for 0.001 for the self_reference_avg_sharess predictor. That is evidence of something worked
```{r}
training_set %>% select(kw_avg_avg, self_reference_avg_sharess, num_hrefs, shares) %>% 
                  filter(kw_avg_avg != 0) %>% 
                  filter(self_reference_avg_sharess != 0) %>% 
                  filter(num_hrefs != 0) %>% 
                   ggpairs()
```



```{r}
tran_predictors = training_set %>% select(kw_avg_avg, self_reference_avg_sharess, 
                                          num_hrefs, shares, popularity)
tran_predictors
```
Observing the dataset, we notice that some obervations have 0 as an value, we treat them as missing values as 0 will not do any good to our predictions. So we further process our training data

```{r}
summary(tran_predictors %>% select(-popularity))
boxplot(tran_predictors %>% select(-popularity, -shares, -num_hrefs), outline = FALSE)
boxplot(tran_predictors$num_hrefs, outline = FALSE)
```


```{r}
tran_predictors = tran_predictors %>% 
                  filter(kw_avg_avg != 0) %>% 
                  filter(self_reference_avg_sharess != 0) %>% 
                  filter(num_hrefs != 0)
tran_predictors
```

We noticed that there are duplicated rows in the tran_predictors, so we want to remove them her. The duplication is probabily caused by some categorical variables that is not selected in train_predictors
```{r}
tran_predictors = tran_predictors %>% distinct()
tran_predictors
```



##### Visualize Explorary

```{r}

eda_kw_rm <- tran_predictors %>% 
                  ggplot(aes(x = kw_avg_avg, y = shares)) +
                  geom_point(alpha = 0.4) +
                  xlab("Averge keyword") +
                  ylab("Number of Shares")
eda_kw_rm

## Remove the outliers
eda_kw_rm_outl <- tran_predictors %>% 
                  filter(popularity != "Viral") %>% 
                  # filter(kw_avg_avg < 10000) %>% 
                  ggplot(aes(x = kw_avg_avg, y = shares)) +
                  geom_point(alpha = 0.4) +
                  xlab("Averge keyword") +
                  ylab("Number of Shares")
eda_kw_rm_outl
## the correlation is small, so it is unlikely we will see a linear pattern
```


```{r}
eda_sf_rm <- tran_predictors %>% 
                  ggplot(aes(x = self_reference_avg_sharess, y = shares)) +
                  geom_point(alpha = 0.4) +
                  xlab("Avg. shares of referenced articles in Mashable") +
                  ylab("Number of Shares")
eda_sf_rm

## Remove the outliers
eda_sf_rm_outl <- tran_predictors %>% 
                  filter(popularity != "Viral") %>%
                  # filter(self_reference_avg_sharess < 2e+05) %>%
                  ggplot(aes(x = self_reference_avg_sharess, y = shares)) +
                  geom_point(alpha = 0.4) +
                  xlab("Avg. shares of referenced articles in Mashable") +
                  ylab("Number of Shares")
eda_sf_rm_outl
## the correlation is small, so it is unlikely we will see a linear pattern
```
```{r}
eda_ref_rm <- tran_predictors %>% 
                  ggplot(aes(x = num_hrefs, y = shares)) +
                  geom_point(alpha = 0.4) +
                  xlab("Number of Links") +
                  ylab("Number of Shares")
eda_ref_rm

## Remove the outliers
eda_ref_rm_outl <- tran_predictors %>% 
                  filter(popularity != "Viral") %>%
                  # filter(self_reference_avg_sharess < 2e+05) %>%
                  ggplot(aes(x = num_hrefs, y = shares)) +
                  geom_point(alpha = 0.4) +
                  ylab("Number of Shares")
eda_ref_rm_outl
## the correlation is small, so it is unlikely we will see a linear pattern
```

A count of the observations in each popularity category


```{r}
## the kappa
tran_predictors %>% group_by(popularity) %>% 
                    summarise(count = n())

```


```{r}
ggplot(tran_predictors, aes(x = popularity, y = kw_avg_avg)) +
geom_boxplot(outlier.shape = NA) +
   ylim(0, 6000)

ggplot(tran_predictors, aes(x = popularity, y = self_reference_avg_sharess)) +
geom_boxplot(outlier.shape = NA) +
   ylim(0, 6000)

ggplot(tran_predictors, aes(x = popularity, y = num_hrefs)) +
geom_boxplot(outlier.shape = NA) +
   ylim(0, 30)
```

### KNN regression analysis

```{r}
## Now we know which predictors we need and what kind of outliers & missing values to remove,
## We retrain our data based on those info

news_processed = news %>% 
                select(kw_avg_avg, self_reference_avg_sharess, num_hrefs, shares) %>% 
                filter(kw_avg_avg != 0) %>% 
                filter(self_reference_avg_sharess != 0) %>% 
                filter(num_hrefs != 0) %>% 
                distinct()
head(news_processed)

## Re-train our data in a regression manner

splitted_data <- initial_split(news_processed, prop = 0.80, strata = shares)  
train <- training(splitted_data)   
test <- testing(splitted_data)
head(train)
head(test)

```


```{r}

news_knn_recipe = recipe(shares ~ ., data = train)

news_knn_recipe

news_knn_spec = nearest_neighbor(weight_func = "rectangular", neighbors = tune()) %>% 
                set_engine("kknn") %>%
                set_mode("regression")

news_knn_spec
```


```{r}
news_vfold = vfold_cv(train, v = 5, strata = shares)

news_vfold

news_knn_workflow = workflow() %>% 
                    add_recipe(news_knn_recipe) %>%
                    add_model(news_knn_spec)

news_knn_workflow

```


```{r}
gridvals = tibble(neighbors = 1:20)

news_knn_results = news_knn_workflow %>% 
                    tune_grid(resamples = news_vfold, grid = gridvals) %>%
                    collect_metrics()

news_knn_results

```


```{r}
knn_rmspe = news_knn_results %>% 
            filter(.metric == "rmse") %>% 
            select(mean) %>% 
            arrange(mean) %>% 
            slice(1) %>% 
            pull()
            
knn_rmspe
```
So we get a knn regression with a RMSPE of 10356.13. We will then try to use linear regression to see which one is better

### Linear Regression
```{r}
lm_spec = linear_reg() %>% 
          set_engine("lm") %>% 
          set_mode("regression")

news_recipe = recipe(shares ~ ., data = train) 

news_lm_fit = workflow() %>% 
        add_recipe(news_recipe) %>% 
        add_model(lm_spec) %>% 
        fit(data = train)

news_lm_fit
```

```{r}
news_lm_pred = news_lm_fit %>% 
              predict(test) %>% 
              bind_cols(test)
news_lm_pred

news_lm_rmspe =  news_lm_pred %>% 
        metrics(truth = shares, estimate = .pred) %>% 
        filter(.metric == "rmse") %>% 
        select(.estimate) %>% 
        pull()
news_lm_rmspe
```

### Compare knn and linear
```{r}
if(knn_rmspe < news_lm_rmspe){
  print("We use knn-regresion as it has a smaller RMSPE")
} else{
  print("we use simple linear regression as it has a smaller RMSPE")
}
knn_rmspe
news_lm_rmspe
```

### Visualization of our prediction

```{r}
lmplot <- lm(shares ~ ., data = news_processed)
summary(lmplot)
plot(lmplot)
## https://rpubs.com/bensonsyd/385183
```

```{r}
lm_kw = news_lm_pred %>% 
        ggplot(aes(x = kw_avg_avg, y = shares)) +
        geom_point(alpha = 0.5) + 
        geom_smooth(method = "lm", se = FALSE) +
        labs(x = "Averge Keyword")
lm_kw
        

lm_kw_zm = news_lm_pred %>% 
        filter(kw_avg_avg < 10000) %>% 
        filter(shares < 1e+05) %>% 
        ggplot(aes(x = kw_avg_avg, y = shares)) +
        geom_point(alpha = 0.5) + 
        geom_smooth(method = "lm", se = FALSE)
lm_kw_zm

```


### Discussion


* Our guess is that the prediction will not work very well as the correlation between our predictors and the response variable shares are extremely low: the highest of the three is around 0.1. Therefore, we are not suprised that ...
* Regardless of our accuracy in predicting the popularity of a facebook share given some attributes of the post, some people is going to be crazy about it. Popular posts ~> influencers or even internet celebrities ~> tons of money ~> no matter how unreliable the methods are and how mere the chances are, they will give it a try. 
* Future questions: Are there any correlations among the predictors themselves? If individually, the predictors have a low correlation with the response variable, how about all together? Or some of them together? 
